{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_openapi_agent\n",
    "from langchain.agents.agent_toolkits import OpenAPIToolkit\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.requests import TextRequestsWrapper\n",
    "from langchain.tools.json.tool import JsonSpec\n",
    "\n",
    "import json, yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"openapi.yaml\") as f:\n",
    "    data = yaml.load(f, Loader=yaml.FullLoader)\n",
    "json_spec = JsonSpec(dict_=data, max_value_length=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api_operation_by_id(json_spec, operation_id):\n",
    "    # Convert the JSON string to a Python dictionary\n",
    "    # spec_dict = json.loads(json_spec)\n",
    "\n",
    "    # Traverse the dictionary to find the operation with the given ID\n",
    "    paths = json_spec.dict_.get(\"paths\", {})\n",
    "    for path, methods in paths.items():\n",
    "        for method, operation in methods.items():\n",
    "            if operation.get(\"operationId\") == operation_id:\n",
    "                return operation\n",
    "\n",
    "    # If the operation ID is not found, return None\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_spec.dict_\n",
    "api_operation=get_api_operation_by_id(json_spec, \"createCompletion\")\n",
    "\n",
    "# api_operation\n",
    "# # api_operation[\"responses\"][\"200\"][\"content\"][\"application/json\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'operationId': 'createCompletion',\n",
       " 'tags': ['OpenAI'],\n",
       " 'summary': 'Creates a completion for the provided prompt and parameters.',\n",
       " 'requestBody': {'required': True,\n",
       "  'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CreateCompletionRequest'}}}},\n",
       " 'responses': {'200': {'description': 'OK',\n",
       "   'content': {'application/json': {'schema': {'$ref': '#/components/schemas/CreateCompletionResponse'}}}}},\n",
       " 'x-oaiMeta': {'name': 'Create completion',\n",
       "  'returns': 'Returns a [completion](/docs/api-reference/completions/object) object, or a sequence of completion objects if the request is streamed.\\n',\n",
       "  'legacy': True,\n",
       "  'examples': [{'title': 'No streaming',\n",
       "    'request': {'curl': 'curl https://api.openai.com/v1/completions \\\\\\n  -H \"Content-Type: application/json\" \\\\\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\\\\n  -d \\'{\\n    \"model\": \"VAR_model_id\",\\n    \"prompt\": \"Say this is a test\",\\n    \"max_tokens\": 7,\\n    \"temperature\": 0\\n  }\\'\\n',\n",
       "     'python': 'import os\\nimport openai\\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\\nopenai.Completion.create(\\n  model=\"VAR_model_id\",\\n  prompt=\"Say this is a test\",\\n  max_tokens=7,\\n  temperature=0\\n)\\n',\n",
       "     'node.js': 'import OpenAI from \"openai\";\\n\\nconst openai = new OpenAI();\\n\\nasync function main() {\\n  const completion = await openai.completions.create({\\n    model: \"VAR_model_id\",\\n    prompt: \"Say this is a test.\",\\n    max_tokens: 7,\\n    temperature: 0,\\n  });\\n\\n  console.log(completion);\\n}\\nmain();'},\n",
       "    'response': '{\\n  \"id\": \"cmpl-uqkvlQyYK7bGYrRHQ0eXlWi7\",\\n  \"object\": \"text_completion\",\\n  \"created\": 1589478378,\\n  \"model\": \"VAR_model_id\",\\n  \"choices\": [\\n    {\\n      \"text\": \"\\\\n\\\\nThis is indeed a test\",\\n      \"index\": 0,\\n      \"logprobs\": null,\\n      \"finish_reason\": \"length\"\\n    }\\n  ],\\n  \"usage\": {\\n    \"prompt_tokens\": 5,\\n    \"completion_tokens\": 7,\\n    \"total_tokens\": 12\\n  }\\n}\\n'},\n",
       "   {'title': 'Streaming',\n",
       "    'request': {'curl': 'curl https://api.openai.com/v1/completions \\\\\\n  -H \"Content-Type: application/json\" \\\\\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\\\\n  -d \\'{\\n    \"model\": \"VAR_model_id\",\\n    \"prompt\": \"Say this is a test\",\\n    \"max_tokens\": 7,\\n    \"temperature\": 0,\\n    \"stream\": true\\n  }\\'\\n',\n",
       "     'python': 'import os\\nimport openai\\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\\nfor chunk in openai.Completion.create(\\n  model=\"VAR_model_id\",\\n  prompt=\"Say this is a test\",\\n  max_tokens=7,\\n  temperature=0,\\n  stream=True\\n):\\n  print(chunk[\\'choices\\'][0][\\'text\\'])\\n',\n",
       "     'node.js': 'import OpenAI from \"openai\";\\n\\nconst openai = new OpenAI();\\n\\nasync function main() {\\n  const stream = await openai.completions.create({\\n    model: \"VAR_model_id\",\\n    prompt: \"Say this is a test.\",\\n    stream: true,\\n  });\\n\\n  for await (const chunk of stream) {\\n    console.log(chunk.choices[0].text)\\n  }\\n}\\nmain();'},\n",
       "    'response': '{\\n  \"id\": \"cmpl-7iA7iJjj8V2zOkCGvWF2hAkDWBQZe\",\\n  \"object\": \"text_completion\",\\n  \"created\": 1690759702,\\n  \"choices\": [\\n    {\\n      \"text\": \"This\",\\n      \"index\": 0,\\n      \"logprobs\": null,\\n      \"finish_reason\": null\\n    }\\n  ],\\n  \"model\": \"text-davinci-003\"\\n}\\n'}]}}"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$ref': '#/components/schemas/CreateCompletionResponse'}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_operation[\"responses\"][\"200\"][\"content\"][\"application/json\"][\"schema\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def resolve_references(input_dict, json_spec):\n",
    "    if not isinstance(input_dict, dict):\n",
    "        return input_dict\n",
    "\n",
    "    if \"$ref\" in input_dict and isinstance(input_dict[\"$ref\"], str):\n",
    "        match = re.match(r'#/components/schemas/(\\w+)', input_dict[\"$ref\"])\n",
    "        if match:\n",
    "            schema_name = match.group(1)\n",
    "            if schema_name in json_spec.get(\"components\", {}).get(\"schemas\", {}):\n",
    "                return json_spec[\"components\"][\"schemas\"][schema_name]\n",
    "\n",
    "    result = {}\n",
    "    for key, value in input_dict.items():\n",
    "        result[key] = resolve_references(value, json_spec)\n",
    "    return result\n",
    "\n",
    "def resolve_references_in_json_spec(json_spec):\n",
    "    for key, value in json_spec.items():\n",
    "        if isinstance(value, dict):\n",
    "            json_spec[key] = resolve_references(value, json_spec)\n",
    "    return json_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'object', 'properties': {'model': {'description': 'ID of the model to use. You can use the [List models](/docs/api-reference/models/list) API to see all of your available models, or see our [Model overview](/docs/models/overview) for descriptions of them.\\n', 'anyOf': [{'type': 'string'}, {'type': 'string', 'enum': ['babbage-002', 'davinci-002', 'text-davinci-003', 'text-davinci-002', 'text-davinci-001', 'code-davinci-002', 'text-curie-001', 'text-babbage-001', 'text-ada-001']}], 'x-oaiTypeLabel': 'string'}, 'prompt': {'description': 'The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays.\\n\\nNote that <|endoftext|> is the document separator that the model sees during training, so if a prompt is not specified the model will generate as if from the beginning of a new document.\\n', 'default': '<|endoftext|>', 'nullable': True, 'oneOf': [{'type': 'string', 'default': '', 'example': 'This is a test.'}, {'type': 'array', 'items': {'type': 'string', 'default': '', 'example': 'This is a test.'}}, {'type': 'array', 'minItems': 1, 'items': {'type': 'integer'}, 'example': '[1212, 318, 257, 1332, 13]'}, {'type': 'array', 'minItems': 1, 'items': {'type': 'array', 'minItems': 1, 'items': {'type': 'integer'}}, 'example': '[[1212, 318, 257, 1332, 13]]'}]}, 'suffix': {'description': 'The suffix that comes after a completion of inserted text.', 'default': None, 'nullable': True, 'type': 'string', 'example': 'test.'}, 'max_tokens': {'type': 'integer', 'minimum': 0, 'default': 16, 'example': 16, 'nullable': True, 'description': \"The maximum number of [tokens](/tokenizer) to generate in the completion.\\n\\nThe token count of your prompt plus `max_tokens` cannot exceed the model's context length. [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb) for counting tokens.\\n\"}, 'temperature': {'type': 'number', 'minimum': 0, 'maximum': 2, 'default': 1, 'example': 1, 'nullable': True, 'description': 'What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\\n\\nWe generally recommend altering this or `top_p` but not both.\\n'}, 'top_p': {'type': 'number', 'minimum': 0, 'maximum': 1, 'default': 1, 'example': 1, 'nullable': True, 'description': 'An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\\n\\nWe generally recommend altering this or `temperature` but not both.\\n'}, 'n': {'type': 'integer', 'minimum': 1, 'maximum': 128, 'default': 1, 'example': 1, 'nullable': True, 'description': 'How many completions to generate for each prompt.\\n\\n**Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.\\n'}, 'stream': {'description': 'Whether to stream back partial progress. If set, tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb).\\n', 'type': 'boolean', 'nullable': True, 'default': False}, 'logprobs': {'type': 'integer', 'minimum': 0, 'maximum': 5, 'default': None, 'nullable': True, 'description': 'Include the log probabilities on the `logprobs` most likely tokens, as well the chosen tokens. For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The API will always return the `logprob` of the sampled token, so there may be up to `logprobs+1` elements in the response.\\n\\nThe maximum value for `logprobs` is 5.\\n'}, 'echo': {'type': 'boolean', 'default': False, 'nullable': True, 'description': 'Echo back the prompt in addition to the completion\\n'}, 'stop': {'description': 'Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.\\n', 'default': None, 'nullable': True, 'oneOf': [{'type': 'string', 'default': '<|endoftext|>', 'example': '\\n', 'nullable': True}, {'type': 'array', 'minItems': 1, 'maxItems': 4, 'items': {'type': 'string', 'example': '[\"\\\\n\"]'}}]}, 'presence_penalty': {'type': 'number', 'default': 0, 'minimum': -2, 'maximum': 2, 'nullable': True, 'description': \"Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\\n\\n[See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details)\\n\"}, 'frequency_penalty': {'type': 'number', 'default': 0, 'minimum': -2, 'maximum': 2, 'nullable': True, 'description': \"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\\n\\n[See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details)\\n\"}, 'best_of': {'type': 'integer', 'default': 1, 'minimum': 0, 'maximum': 20, 'nullable': True, 'description': 'Generates `best_of` completions server-side and returns the \"best\" (the one with the highest log probability per token). Results cannot be streamed.\\n\\nWhen used with `n`, `best_of` controls the number of candidate completions and `n` specifies how many to return â€“ `best_of` must be greater than `n`.\\n\\n**Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.\\n'}, 'logit_bias': {'type': 'object', 'x-oaiTypeLabel': 'map', 'default': None, 'nullable': True, 'additionalProperties': {'type': 'integer'}, 'description': 'Modify the likelihood of specified tokens appearing in the completion.\\n\\nAccepts a json object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this [tokenizer tool](/tokenizer?view=bpe) (which works for both GPT-2 and GPT-3) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\\n\\nAs an example, you can pass `{\"50256\": -100}` to prevent the <|endoftext|> token from being generated.\\n'}, 'user': {'type': 'string', 'example': 'user-1234', 'description': 'A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).\\n'}}, 'required': ['model', 'prompt']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Resolve references within the json_spec dictionary\n",
    "json_spec = resolve_references_in_json_spec(json_spec.dict_)\n",
    "\n",
    "input_dict = api_operation[\"requestBody\"][\"content\"][\"application/json\"][\"schema\"]\n",
    "\n",
    "result = resolve_references(input_dict, json_spec)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
